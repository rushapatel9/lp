from google.colab import drive
drive.mount('/content/drive')

#1
import tensorflow as tf
from sklearn.preprocessing import StandardScaler
from sklearn import metrics
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the Boston Housing dataset
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.boston_housing.load_data(path='boston_housing.npz', test_split=0.2, seed=42)

# Normalize feature data
scaler = StandardScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_test_scaled = scaler.transform(x_test)

# Build the deep neural network model
model = tf.keras.models.Sequential([
    tf.keras.layers.Input(shape=(13,)),
    tf.keras.layers.Dense(100, activation='relu'),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dense(50, activation='relu'),
    tf.keras.layers.Dense(1)  # Linear activation by default
])
# Compile the model
model.compile(optimizer='adam', loss='mse', metrics=['mae'])
# Train the model
history = model.fit(x_train_scaled, y_train, batch_size=32, epochs=20, validation_data=(x_test_scaled, y_test))
# Plot training history
pd.DataFrame(history.history).plot(figsize=(10, 6))
plt.title("Training & Validation Loss/MAE")
plt.show()
# Predict on test data
y_pred = model.predict(x_test_scaled)
# Plot predicted vs actual
sns.regplot(x=y_test, y=y_pred.flatten(), line_kws={"color": "red"})
plt.xlabel("Actual Price")
plt.ylabel("Predicted Price")
plt.title("Actual vs Predicted Prices")
plt.show()
# Print regression metrics
def regression_metrics_display(y_true, y_pred):
    print(f"MAE: {metrics.mean_absolute_error(y_true, y_pred)}")
    print(f"MSE: {metrics.mean_squared_error(y_true, y_pred)}")
    print(f"R² Score: {metrics.r2_score(y_true, y_pred)}")
regression_metrics_display(y_test, y_pred)

#2Import necessary libraries to carry out this classification
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt
import keras
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()
class_names = ['T_shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal','Shirt', 'Sneaker', 'Bag', 'Ankle boot']
print("Number of images in training set {}".format(x_train.shape))
print("Number of labels in training set {}".format(y_train.shape))
print("Number of images in test set {}".format(x_test.shape))
print("Number of labels in test set {}".format(y_train.shape))
plt.figure()
plt.imshow(np.squeeze(x_train[220]))
y_train[220]
# Let us plot some training images to see how they look
plt.figure(figsize=(10,10))
for i in range(15):
  plt.subplot(5,5,i+1)
    plt.xticks([])
    plt.yticks([])
    plt.imshow(x_train[i], cmap=plt.cm.binary)
    plt.xlabel(class_names[y_train[i]])
plt.show()
x_train=x_train/255
x_test=x_test/255
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import TensorBoard
cnn_model = Sequential()
cnn_model.add(Conv2D(32,3,3,input_shape = (28,28,1),activation = 'relu'))
cnn_model.add(MaxPooling2D(pool_size= (2,2)))
cnn_model.add(Flatten())
cnn_model.add(Dense(32,activation = 'relu'))
cnn_model.add(Dense(10,activation = 'sigmoid'))
cnn_model.summary()cnn_model.compile(loss ='sparse_categorical_crossentropy',optimizer = Adam(learning_rate=0.001),
metrics= ['accuracy'])
history=cnn_model.fit(x_train,y_train,batch_size =512,epochs = 5,verbose = 1,validation_data = (x_test,y_test) )
cnn_model.evaluate(x_test, y_test)
probability_model = tf.keras.Sequential([cnn_model, tf.keras.layers.Softmax()])
predictions = probability_model.predict(x_test)
img = x_test[6]
plt.imshow(img)
y_predict = class_names[np.argmax(predictions[6])]
y_predict
y_actual = class_names[y_test[6]]
y_actual
img = x_test[0]
plt.imshow(img)
y_predict = class_names[np.argmax(predictions[0])]
y_predict
y_actual = class_names[y_test[0]]
y_actual
test_loss, test_accuracy = cnn_model.evaluate(x_train, y_train)
print(test_accuracy)
print(test_loss)
history.history.keys()
plt.plot(history.history['accuracy'],label='Accuracy')
plt.plot(history.history['val_accuracy'])
plt.title("Accuracy vs Validation Accuracy")
plt.xlabel("No. of epoch")
plt.ylabel("Accuracy")
plt.legend(['Train_acc', 'Val_acc'], loc='lower right')
plt.show()
from mlxtend.plotting import plot_confusion_matrix
from sklearn.metrics import confusion_matrix  # Import the confusion_matrix function from sklearn.metrics
# Generate confusion matrix
cm = confusion_matrix(y_test, np.argmax(predictions, axis=1))
plot_confusion_matrix(cm, figsize=(10, 7), class_names=class_names)
plt.title("Confusion Matrix")
plt.show()

#3
# Install yfinance library to fetch stock data programmatically
!pip install yfinance

# Import required libraries
import yfinance as yf
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

#new cell
# Download Google stock data for training (2012-2017)
data = yf.download('GOOG', start='2012-01-01', end='2017-12-31')

# Use only the 'Open' column for prediction
dataset_train = data[['Open']]
training_set = dataset_train.values

#new cell
# Normalize the stock prices to a range between 0 and 1
from sklearn.preprocessing import MinMaxScaler
sc = MinMaxScaler(feature_range=(0, 1))
training_set_scaled = sc.fit_transform(training_set)

# Create data structure with 60 timesteps and 1 output
X_train = []
y_train = []
for i in range(60, len(training_set)):
    X_train.append(training_set_scaled[i-60:i, 0])  # 60 previous stock prices
    y_train.append(training_set_scaled[i, 0])       # price at time i to be predicted

# Convert to NumPy arrays and reshape for LSTM input
X_train, y_train = np.array(X_train), np.array(y_train)
X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))  # [samples, timesteps, features]

#new cell
from keras.models import Sequential
from keras.layers import Dense, LSTM, Dropout

# Initialize the RNN
regressor = Sequential()

# Add 4 LSTM layers with dropout to prevent overfitting
regressor.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))
regressor.add(Dropout(0.2))

regressor.add(LSTM(units=50, return_sequences=True))
regressor.add(Dropout(0.2))

regressor.add(LSTM(units=50, return_sequences=True))
regressor.add(Dropout(0.2))

regressor.add(LSTM(units=50))
regressor.add(Dropout(0.2))

# Output layer
regressor.add(Dense(units=1))

# Compile the model
regressor.compile(optimizer='adam', loss='mean_squared_error')

#new cell
# Train the model on the training data
regressor.fit(X_train, y_train, epochs=20, batch_size=32)

#newcell
# Download test data for Google (Jan 2018 to Mar 2018)
data_test = yf.download('GOOG', start='2018-01-01', end='2018-03-01')
dataset_test = data_test[['Open']]
real_stock_price = dataset_test.values

# Combine train and test data for creating input sequences
dataset_total = pd.concat((dataset_train['Open'], dataset_test['Open']), axis=0)
inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values

# Reshape and scale inputs
inputs = inputs.reshape(-1, 1)
inputs = sc.transform(inputs)

#new cell
# Create test sequences using 60 timesteps
X_test = []
for i in range(60, 60 + len(dataset_test)):
    X_test.append(inputs[i-60:i, 0])
X_test = np.array(X_test)
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

#new cell
# Predict stock prices using the trained RNN
predicted_stock_price = regressor.predict(X_test)

# Reverse the scaling to get actual predicted prices
predicted_stock_price = sc.inverse_transform(predicted_stock_price)

# Plot real vs predicted prices
plt.plot(real_stock_price, color='red', label='Real Google Stock Price')
plt.plot(predicted_stock_price, color='blue', label='Predicted Google Stock Price')
plt.title('Google Stock Price Prediction')
plt.xlabel('Time')
plt.ylabel('Google Stock Price')
plt.legend()
plt.show()




#classification

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical

# Load the dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/letter-recognition/letter-recognition.data"
column_names = ['letter','x-box','y-box','width','height','onpix','x-bar','y-bar',
                'x2bar','y2bar','xybar','x2ybr','xy2br','x-ege','xegvy','y-ege','yegvx']
df = pd.read_csv(url, names=column_names)

# Encode target labels (A-Z → 0-25)
label_encoder = LabelEncoder()
y = label_encoder.fit_transform(df['letter'])
y_cat = to_categorical(y)  # One-hot encoding

# Features and scaling
X = df.drop('letter', axis=1)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_cat, test_size=0.2, random_state=42)

#new cell
model = Sequential()
model.add(Dense(128, input_shape=(16,), activation='relu'))
model.add(Dropout(0.3))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.2))
model.add(Dense(26, activation='softmax'))  # 26 classes

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

#new cell
#train the model
history = model.fit(X_train, y_train, epochs=30, batch_size=64, validation_split=0.1)

#new cell
#Evaluate the model
test_loss, test_acc = model.evaluate(X_test, y_test)
print(f'Test accuracy: {test_acc:.2f}')


#new cell
#training history
import matplotlib.pyplot as plt

plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Val Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Training & Validation Accuracy')
plt.show()
